{
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 25,
      "outputs": [],
      "metadata": {},
      "source": [
        "from pyspark.sql import SparkSession\r\n",
        "from pyspark.sql.functions import col, current_date, lit, coalesce\r\n",
        "from pyspark.sql.types import IntegerType, StringType\r\n",
        "\r\n",
        "spark = SparkSession.builder.appName(\"SCD_DIM_TBL\").getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "silver_dataset_folder_path = \"abfss://silver@adlssalesproject2448pp2.dfs.core.windows.net\"\r\n",
        "gold_dataset_folder_path = \"abfss://gold@adlssalesproject2448pp2.dfs.core.windows.net\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "pk_dict = {\r\n",
        "    'PRODUCT': 'NDC_CD',\r\n",
        "    'PROVIDER': 'PROVIDER_ID',\r\n",
        "    'DIAGNOSIS': 'DIAGNOSIS_CODE,ICD_VERSION_TYPE',\r\n",
        "    'PROCEDURE': 'PROCEDURE_CODE,PRC_VERS_TYP_ID',\r\n",
        "    'PLAN': 'PAYER_PLAN_ID',\r\n",
        "    'RX PATIENT ACTIVITY': 'PATIENT_ID',\r\n",
        "    'PATIENT MPD': 'PATIENT_ID,MPD_YEAR',\r\n",
        "    'PATIENT COMMERCIAL': 'PATIENT_ID,ACTIVITY_YEAR',\r\n",
        "    'PATIENT DEMOGRAPHICS': 'PATIENT_ID'\r\n",
        "}\r\n",
        "scd_dict = {\r\n",
        "    'PRODUCT': 'SCD2',\r\n",
        "    'PROVIDER': 'SCD2',\r\n",
        "    'DIAGNOSIS': 'SCD1',\r\n",
        "    'PROCEDURE': 'SCD1',\r\n",
        "    'PLAN': 'SCD1',\r\n",
        "    'RX PATIENT ACTIVITY': 'SCD1',\r\n",
        "    'PATIENT MPD': 'SCD1',\r\n",
        "    'PATIENT COMMERCIAL': 'SCD1',\r\n",
        "    'PATIENT DEMOGRAPHICS': 'SCD1'\r\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Get subfolders in the silver dataset\r\n",
        "silver_subfolder_list = mssparkutils.fs.ls(silver_dataset_folder_path)\r\n",
        "\r\n",
        "for subfolder in silver_subfolder_list:\r\n",
        "    subfolder_path = subfolder.path\r\n",
        "    subfolder_name = subfolder.name\r\n",
        "    \r\n",
        "    # Extract the prefix to determine table name\r\n",
        "    f_name = subfolder_name.split('_')[0]  # Corrected this line to get the table name\r\n",
        "    pk_cols = pk_dict[f_name].split(',')   # Split primary key columns in case there are multiple keys\r\n",
        "    \r\n",
        "    # Get the list of files in the subfolder\r\n",
        "    file_list = mssparkutils.fs.ls(subfolder_path)\r\n",
        "    \r\n",
        "    for file in file_list:\r\n",
        "        if file.size != 0:\r\n",
        "            file_path = file.path\r\n",
        "            silver_df = spark.read.option(\"header\", True).parquet(file_path)\r\n",
        "            \r\n",
        "            try:\r\n",
        "                # Load existing gold table\r\n",
        "                gold_df = spark.read.format(\"delta\").load(f\"{gold_dataset_folder_path}/{f_name}\")\r\n",
        "            except:\r\n",
        "                # Initialize an empty gold dataframe with required schema if the gold table doesn't exist\r\n",
        "                gold_df = spark.createDataFrame([], silver_df.schema.add(\"Effective_date\", StringType())\r\n",
        "                                                         .add(\"ValidTo\", StringType())\r\n",
        "                                                         .add(\"ActiveFlag\", StringType())\r\n",
        "                                                         .add(\"Version\", IntegerType()))\r\n",
        "\r\n",
        "            if scd_dict[f_name].lower() == 'scd1':\r\n",
        "                # Perform SCD Type 1 logic - overwrite records\r\n",
        "                joined_df = silver_df.alias(\"src\").join(gold_df.alias(\"dest\"), pk_cols, \"left\")\r\n",
        "                \r\n",
        "                # If record doesn't exist in the target, insert new data, otherwise update with latest values\r\n",
        "                final_df = joined_df.selectExpr(\"src.*\", \r\n",
        "                                                \"coalesce(dest.Effective_date, cast(current_date() as string)) as Effective_date\")\r\n",
        "                \r\n",
        "                safe_f_name = f_name.replace(\" \",\"_\")\r\n",
        "\r\n",
        "                # Save in Delta format (overwrite mode for SCD1)\r\n",
        "                final_df.coalesce(1).write.format(\"delta\").mode(\"overwrite\").save(f\"{gold_dataset_folder_path}/{safe_f_name}_scd1\")\r\n",
        "                \r\n",
        "                # Create Delta table in the database (corrected SQL query)\r\n",
        "                \r\n",
        "                spark.sql(f\"CREATE DATABASE IF NOT EXISTS scdprojectdb;\")\r\n",
        "\r\n",
        "                spark.sql(f\"\"\"\r\n",
        "                    CREATE TABLE IF NOT EXISTS scdprojectdb.`TBL_DIM_{safe_f_name}_scd1`\r\n",
        "                    USING DELTA\r\n",
        "                    LOCATION '{gold_dataset_folder_path}/{safe_f_name}_scd1';\r\n",
        "                \"\"\")\r\n",
        "                spark.sql(f\"\"\"\r\n",
        "                select * from scdprojectdb.`TBL_DIM_{safe_f_name}_scd1` LIMIT 1;\r\n",
        "                \"\"\")\r\n",
        "\r\n",
        "            elif scd_dict[f_name].lower() == 'scd2':\r\n",
        "                # Perform SCD Type 2 logic - maintain history\r\n",
        "                join_condition = ' AND '.join([f\"src.{col} = dest.{col}\" for col in pk_cols])\r\n",
        "\r\n",
        "                # Identify new records\r\n",
        "                new_records_df = silver_df.alias(\"src\") \\\r\n",
        "                    .join(gold_df.alias(\"dest\"), pk_cols, \"left_anti\") \\\r\n",
        "                    .selectExpr(\"src.*\", \r\n",
        "                                \"current_date() as Effective_date\", \r\n",
        "                                \"NULL as ValidTo\", \r\n",
        "                                \"'Y' as ActiveFlag\", \r\n",
        "                                \"1 as Version\")\r\n",
        "                \r\n",
        "\r\n",
        "                # Identify updated records\r\n",
        "                updated_records_df = silver_df.alias(\"src\") \\\r\n",
        "                    .join(gold_df.alias(\"dest\"), pk_cols, \"inner\") \\\r\n",
        "                    .filter(\"dest.ActiveFlag = 'Y' AND \" + \" OR \".join([f\"src.{col} != dest.{col}\" for col in silver_df.columns if col not in[\"Effective_date\",\"ValidTo\",\"ActiveFlag\",\"Version\"]])) \\\r\n",
        "                    .selectExpr(\"src.*\", \r\n",
        "                                \"current_date() as Effective_date\", \r\n",
        "                                \"NULL as ValidTo\", \r\n",
        "                                \"'Y' as ActiveFlag\", \r\n",
        "                                \"dest.Version + 1 as Version\")\r\n",
        "                \r\n",
        "\r\n",
        "                # Inactivate old records in the gold_df\r\n",
        "                gold_inactive_df = silver_df.alias(\"src\") \\\r\n",
        "                    .join(gold_df.alias(\"dest\"), pk_cols, \"inner\") \\\r\n",
        "                    .filter(\"dest.ActiveFlag = 'Y' AND \" + \" OR \".join([f\"src.{col} != dest.{col}\" for col in silver_df.columns if col not in[\"Effective_date\",\"ValidTo\",\"ActiveFlag\",\"Version\"]])) \\\r\n",
        "                    .selectExpr(\"dest.*\") \\\r\n",
        "                    .withColumn(\"ValidTo\", current_date()) \\\r\n",
        "                    .withColumn(\"ActiveFlag\", lit(\"N\"))\r\n",
        "                \r\n",
        "                # Combine all records (new, updated, and inactivated)\r\n",
        "                final_df = new_records_df.union(updated_records_df).union(gold_inactive_df)\r\n",
        "\r\n",
        "                safe_f_name = f_name.replace(\" \",\"_\")\r\n",
        "\r\n",
        "                # Save in Delta format (append mode for SCD2)\r\n",
        "                final_df.coalesce(1).write.format(\"delta\").mode(\"overwrite\").save(f\"{gold_dataset_folder_path}/{safe_f_name}_scd2\")\r\n",
        "                \r\n",
        "                # Create Delta table in the database (corrected SQL query)\r\n",
        "                \r\n",
        "                spark.sql(f\"CREATE DATABASE IF NOT EXISTS scdprojectdb;\")\r\n",
        "\r\n",
        "                spark.sql(f\"\"\"\r\n",
        "                    CREATE TABLE IF NOT EXISTS scdprojectdb.`TBL_DIM_{safe_f_name}_scd2`\r\n",
        "                    USING DELTA\r\n",
        "                    LOCATION '{gold_dataset_folder_path}/{safe_f_name}_scd2';\r\n",
        "                \"\"\")\r\n",
        ""
      ]
    }
  ],
  "metadata": {
    "description": null,
    "save_output": true,
    "language_info": {
      "name": "python"
    }
  }
}