{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "python"
    },
    "description": null,
    "save_output": true,
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "SparkPool2448PP",
              "statement_id": 14,
              "statement_ids": [
                14
              ],
              "state": "finished",
              "livy_statement_state": "available",
              "spark_jobs": null,
              "session_id": "27",
              "normalized_state": "finished",
              "queued_time": "2024-09-22T12:11:54.0005489Z",
              "session_start_time": null,
              "execution_start_time": "2024-09-22T12:11:54.1724689Z",
              "execution_finish_time": "2024-09-22T12:11:54.3932104Z",
              "parent_msg_id": "26b4ce90-9048-4241-b0a3-3c21a34a3111"
            },
            "text/plain": "StatementMeta(SparkPool2448PP, 27, 14, Finished, Available, Finished)"
          },
          "metadata": {}
        }
      ],
      "metadata": {},
      "source": [
        "from pyspark.sql import SparkSession\r\n",
        "from pyspark.sql.functions import col, count\r\n",
        "\r\n",
        "# Initialize the Spark session\r\n",
        "spark = SparkSession.builder.appName(\"Data_Validation_PrimaryKey\").getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "SparkPool2448PP",
              "statement_id": 15,
              "statement_ids": [
                15
              ],
              "state": "finished",
              "livy_statement_state": "available",
              "spark_jobs": null,
              "session_id": "27",
              "normalized_state": "finished",
              "queued_time": "2024-09-22T12:11:54.1094998Z",
              "session_start_time": null,
              "execution_start_time": "2024-09-22T12:11:54.5351947Z",
              "execution_finish_time": "2024-09-22T12:11:54.7128786Z",
              "parent_msg_id": "ab789dc9-e0e9-4abe-89bb-d9160f2e0e20"
            },
            "text/plain": "StatementMeta(SparkPool2448PP, 27, 15, Finished, Available, Finished)"
          },
          "metadata": {}
        }
      ],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Define the dataset folder paths\r\n",
        "bronze_dataset_folder_path = \"abfss://bronze@adlssalesproject2448pp2.dfs.core.windows.net\"\r\n",
        "silver_dataset_folder_path = \"abfss://silver@adlssalesproject2448pp2.dfs.core.windows.net\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "SparkPool2448PP",
              "statement_id": 16,
              "statement_ids": [
                16
              ],
              "state": "finished",
              "livy_statement_state": "available",
              "spark_jobs": null,
              "session_id": "27",
              "normalized_state": "finished",
              "queued_time": "2024-09-22T12:11:54.2410264Z",
              "session_start_time": null,
              "execution_start_time": "2024-09-22T12:11:54.8572762Z",
              "execution_finish_time": "2024-09-22T12:11:55.0651088Z",
              "parent_msg_id": "1be14a1d-154a-46dd-a818-a98698c1b4cc"
            },
            "text/plain": "StatementMeta(SparkPool2448PP, 27, 16, Finished, Available, Finished)"
          },
          "metadata": {}
        }
      ],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Define the primary key dictionary\r\n",
        "pk_dict = {\r\n",
        "    'PRODUCT': 'NDC_CD',\r\n",
        "    'PROVIDER': 'PROVIDER_ID',\r\n",
        "    'DIAGNOSIS': 'DIAGNOSIS_CODE,ICD_VERSION_TYPE',\r\n",
        "    'PROCEDURE': 'PROCEDURE_CODE,PRC_VERS_TYP_ID',\r\n",
        "    'PLAN': 'PAYER_PLAN_ID',\r\n",
        "    'RX PATIENT ACTIVITY': 'PATIENT_ID',\r\n",
        "    'PATIENT MPD': 'PATIENT_ID,MPD_YEAR',\r\n",
        "    'PATIENT COMMERCIAL': 'PATIENT_ID,ACTIVITY_YEAR',\r\n",
        "    'PATIENT DEMOGRAPHICS': 'PATIENT_ID'\r\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "SparkPool2448PP",
              "statement_id": 17,
              "statement_ids": [
                17
              ],
              "state": "finished",
              "livy_statement_state": "available",
              "spark_jobs": null,
              "session_id": "27",
              "normalized_state": "finished",
              "queued_time": "2024-09-22T12:11:54.5147807Z",
              "session_start_time": null,
              "execution_start_time": "2024-09-22T12:11:55.2358686Z",
              "execution_finish_time": "2024-09-22T12:12:12.2759429Z",
              "parent_msg_id": "cf24b3db-43f9-4196-9da5-9093e85080cd"
            },
            "text/plain": "StatementMeta(SparkPool2448PP, 27, 17, Finished, Available, Finished)"
          },
          "metadata": {}
        }
      ],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# List all subfolders within the bronze dataset folder\r\n",
        "bronze_subfolder_list = mssparkutils.fs.ls(bronze_dataset_folder_path)\r\n",
        "\r\n",
        "for subfolder in bronze_subfolder_list:\r\n",
        "    subfolder_path = subfolder.path\r\n",
        "    \r\n",
        "    # List all files within the subfolder\r\n",
        "    file_list = mssparkutils.fs.ls(subfolder_path)\r\n",
        "    \r\n",
        "    for file in file_list:\r\n",
        "        file_path = file.path\r\n",
        "        \r\n",
        "        # Read the file into a Spark DataFrame\r\n",
        "        df = spark.read.csv(file_path, header=True, inferSchema=True)\r\n",
        "        \r\n",
        "        # Get the primary key column(s) for the current subfolder\r\n",
        "        pk_columns = pk_dict[subfolder.name]\r\n",
        "        pk_col_list = pk_columns.split(\",\")  # Split by comma for composite keys\r\n",
        "        \r\n",
        "        # Count the number of primary key columns\r\n",
        "        pk_col_count = len(pk_col_list)\r\n",
        "        \r\n",
        "        # Filter rows where any of the primary key columns are null\r\n",
        "        null_pk_df = df.filter(\r\n",
        "            col(pk_col_list[0]).isNull() if pk_col_count == 1 else \r\n",
        "            (col(pk_col_list[0]).isNull() | col(pk_col_list[1]).isNull())\r\n",
        "        )\r\n",
        "        \r\n",
        "        # Find duplicates based on primary key columns\r\n",
        "        if pk_col_count == 1:\r\n",
        "            duplicate_pk_df = df.groupBy(pk_col_list[0]).agg(count(\"*\").alias(\"count\")).filter(col(\"count\") > 1)\r\n",
        "        else:\r\n",
        "            duplicate_pk_df = df.groupBy(pk_col_list[0], pk_col_list[1]).agg(count(\"*\").alias(\"count\")).filter(col(\"count\") > 1)\r\n",
        "        \r\n",
        "        duplicate_pk_df = duplicate_pk_df.drop('count')\r\n",
        "\r\n",
        "        # Combine null primary key rows and duplicate primary key rows\r\n",
        "        reject_df = null_pk_df.union(df.join(duplicate_pk_df, on=pk_col_list, how=\"inner\"))\r\n",
        "        \r\n",
        "        # Subtract rejected rows from the original DataFrame to get valid rows\r\n",
        "        valid_df = df.subtract(reject_df)\r\n",
        "        \r\n",
        "        # Write valid data to the silver folder path\r\n",
        "        valid_path = f\"{silver_dataset_folder_path}/{file.name.replace('.csv', '.parquet')}\"\r\n",
        "        valid_df.coalesce(1).write.mode(\"overwrite\").parquet(valid_path)\r\n",
        "        \r\n",
        "        # If there are any rejected rows, write them to a separate reject path\r\n",
        "        if reject_df.count() > 0:\r\n",
        "            reject_path = f\"{silver_dataset_folder_path}/Reject_{file.name.replace('.csv', '.parquet')}\"\r\n",
        "            reject_df.coalesce(1).write.mode(\"overwrite\").parquet(reject_path)\r\n",
        "        \r\n",
        "        # Free up memory for the intermediate DataFrames\r\n",
        "        reject_df.unpersist()\r\n",
        "        valid_df.unpersist()\r\n",
        "        duplicate_pk_df.unpersist()\r\n",
        "        null_pk_df.unpersist()\r\n",
        "        df.unpersist()\r\n",
        ""
      ]
    }
  ]
}