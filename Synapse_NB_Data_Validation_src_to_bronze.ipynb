{
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "from pyspark.sql import SparkSession\r\n",
        "from pyspark.sql.functions import lit, col\r\n",
        "from pyspark.sql import DataFrame\r\n",
        "\r\n",
        "# create spark session\r\n",
        "spark=SparkSession.builder.appName(\"Data_Validation\").getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "data_path = \"abfss://source@adlssalesproject2448pp1.dfs.core.windows.net\"\r\n",
        "control_file_path = \"abfss://source@adlssalesproject2448pp1.dfs.core.windows.net/CONTROL FILES.csv\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "try:\r\n",
        "    control_df = spark.read.csv(control_file_path, header=True, inferSchema=True)\r\n",
        "    # Show control data to check if it's read correctly\r\n",
        "    control_df.show()\r\n",
        "except Exception as e:\r\n",
        "    print(f\"Error reading control file: {e}\")\r\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "if control_df is None or control_df.head(1) == []:\r\n",
        "    raise ValueError(\"Control file is empty or not read correctly\")\r\n",
        "\r\n",
        "# Define schema for control file with filename and expected row count\r\n",
        "control_df = control_df.withColumnRenamed(\"File Name\", \"file_name\").withColumnRenamed(\"Record count\", \"expected_row_count\")\r\n",
        "control_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "control_df.printSchema()\r\n",
        "print(control_df.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "validation_results = []\r\n",
        "\r\n",
        "for row in control_df.collect():\r\n",
        "    file_name = row['File Name ']\r\n",
        "    expected_row_count = int(row['expected_row_count'])\r\n",
        "\r\n",
        "    file_path = f\"{data_path}/{file_name}\"\r\n",
        "\r\n",
        "    try:\r\n",
        "        df=spark.read.csv(file_path, header=True)\r\n",
        "        row_count = df.count()\r\n",
        "\r\n",
        "        if row_count == expected_row_count:\r\n",
        "            validation_results.append((file_name, row_count, \"Row count Matched\"))\r\n",
        "        else:\r\n",
        "            validation_results.append((file_name, row_count, \"Row count NOT Matched\"))\r\n",
        "    \r\n",
        "    except Exception as e:\r\n",
        "        validation_results.append((file_name, None, f\"ERROR: {str(e)}\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "print(validation_results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "result_df = spark.createDataFrame(validation_results, [\"file_name\", \"actual_count\", \"status\"])\r\n",
        "result_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "output_path = \"abfss://validation@adlssalesproject2448pp2.dfs.core.windows.net/Output.csv\"\r\n",
        "result_df.coalesce(1).write.mode(\"overwrite\").csv(output_path, header=True, mode='overwrite')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "error_df = result_df.filter(col(\"Status\") != \"Row count Matched\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "if error_df.count() > 0:\r\n",
        "    error_log_path = \"abfss://validation@adlssalesproject2448pp2.dfs.core.windows.net/ErrorLog.csv\"\r\n",
        "    error_df.coalesce(1).write.mode(\"overwrite\").csv(error_log_path, header=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "import smtplib\r\n",
        "from email.mime.multipart import MIMEMultipart\r\n",
        "from email.mime.text import MIMEText\r\n",
        "import pandas as pd\r\n",
        "\r\n",
        "if 'error_df' in locals() and error_df.count() > 0:\r\n",
        "    # Convert the Spark DataFrame to Pandas for easy manipulation\r\n",
        "    error_pdf = error_df.toPandas()\r\n",
        "\r\n",
        "    # Convert the DataFrame to an HTML table (or string)\r\n",
        "    html_table = error_pdf.to_html(index=False)  # Use .to_string() if you prefer plain text\r\n",
        "\r\n",
        "    # Email parameters\r\n",
        "    sender_email = \"potghanpramod13@gmail.com\"\r\n",
        "    receiver_email = \"pppotghan@gmail.com\"\r\n",
        "    subject = \"Error DataFrame Report\"\r\n",
        "    smtp_server = \"smtp.gmail.com\"  # Example: \"smtp.gmail.com\" for Gmail\r\n",
        "    smtp_port = 587  # For Gmail, it's 587 for TLS\r\n",
        "    password = \"Potghanp13@\"  # Use your app password here\r\n",
        "\r\n",
        "    # Create the email body in HTML format\r\n",
        "    msg = MIMEMultipart()\r\n",
        "    msg['From'] = sender_email\r\n",
        "    msg['To'] = receiver_email\r\n",
        "    msg['Subject'] = subject\r\n",
        "\r\n",
        "    # Add the DataFrame as part of the email body\r\n",
        "    body = f\"\"\"\r\n",
        "    <html>\r\n",
        "        <body>\r\n",
        "            <p>Hello Sir/Ma'am,</p>\r\n",
        "            <p>Kindly find below details of errors in Data Validation Phase as you requested:</p>\r\n",
        "            {html_table}\r\n",
        "            <p>Regards,<br>Data Engineer Team</br></p>\r\n",
        "        </body>\r\n",
        "    </html>\r\n",
        "    \"\"\"\r\n",
        "    msg.attach(MIMEText(body, 'html'))\r\n",
        "\r\n",
        "    # Send the email\r\n",
        "    try:\r\n",
        "        server = smtplib.SMTP(smtp_server, smtp_port)\r\n",
        "        server.starttls()  # Secure the connection\r\n",
        "        server.login(sender_email, password)\r\n",
        "        text = msg.as_string()\r\n",
        "        server.sendmail(sender_email, receiver_email, text)\r\n",
        "        server.quit()\r\n",
        "        print(\"Email sent successfully!\")\r\n",
        "    except Exception as e:\r\n",
        "        print(f\"Failed to send email. Error: {str(e)}\")\r\n",
        "else:\r\n",
        "    print(\"No errors to report.\")\r\n",
        ""
      ]
    }
  ],
  "metadata": {
    "description": null,
    "save_output": true,
    "language_info": {
      "name": "python"
    }
  }
}